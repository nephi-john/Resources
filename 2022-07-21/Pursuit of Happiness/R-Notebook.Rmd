---
title: "R Notebook"
output:
  html_document: 
    toc: yes
    fig_caption: yes
    keep_md: yes
  pdf_document: default
---

```{r}
# CHANGE PATH HERE:

mypath = "D:\\_nj_\\-github-\\Resources\\2022-07-21\\Pursuit of Happiness\\";


# YOU need to install these packages for this to work.
library(SentimentAnalysis);
library(wordcloud2);
library(wordcloud);
library(tm);
library(textclean);
library(slam);
```

# Inputs

I am comparing the ORIG draft of the Declaration of Independence to the COPY that was approved by Congress after imposing edits.

https://www.ushistory.org/declaration/document/compare.html

## [Slavery challenged] ORIG

```{r}

myFile = paste0(mypath,"ORIG.txt");
ORIG = readChar(myFile, file.info(myFile)$size);

```



## [Slavery ignored] COPY

```{r}

myFile = paste0(mypath,"COPY.txt");
COPY = readChar(myFile, file.info(myFile)$size);
```


# COMPARISONS

## Sentiment

Is the message negative or positive?

```{r}
# https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html

sentiment = analyzeSentiment( c(ORIG,COPY) );
rownames(sentiment) = c("ORIG", "COPY");
sentiment;

```

## PREP using tm

```{r}
# https://riptutorial.com/r/example/31050/create-a-term-frequency-matrix
# docs = Corpus(VectorSource( c(ORIG,COPY) ));



prepString = function(vec, stemMe=FALSE)
{
docs = Corpus(VectorSource( vec ));

docs = tm_map(docs, replace_non_ascii);

# function from textclean to remove curly quotes ” and ’
docs = tm_map(docs, replace_curly_quote);
# function from textclean to replace "it's" to "it is"
docs = tm_map(docs, replace_contraction);




docs = tm_map(docs,removeNumbers);
docs = tm_map(docs,removePunctuation);
docs = tm_map(docs,stripWhitespace);
docs = tm_map(docs, PlainTextDocument);

docs = tm_map(docs,content_transformer(tolower));
docs = tm_map(docs,removeWords, stopwords("english"));



if(stemMe)
{
docs <- tm_map(docs, stemDocument, language="english");
}


# inspect(docs);

docs;
}

```


```{r}
doc.ORIG = prepString(c(ORIG));
doc.COPY= prepString(c(COPY));
doc.BOTH = prepString(c(ORIG,COPY));
```


## WordCloud "RAW"
```{r}

buildFreq = function(docs)
{
dtm     = TermDocumentMatrix(docs);
words   = sort(rowSums(as.matrix(dtm)),decreasing=TRUE);
words.f = data.frame(word = names(words),freq=words);

words.f;
}

# ORIG.freq = termFreq(inspect(docs[1])$content); # ORIG
# COPY.freq = termFreq(inspect(docs[1])$content); # COPY
```

### ORIG
```{r}
ORIG.freq = buildFreq(doc.ORIG);
set.seed(1234);
wordcloud2(ORIG.freq);
```

### COPY
```{r}
COPY.freq = buildFreq(doc.COPY);
set.seed(1234);
wordcloud2(COPY.freq);
```


## STEMMED
```{r}
doc.ORIG.s = prepString(c(ORIG), stemMe=TRUE);
doc.COPY.s= prepString(c(COPY), stemMe=TRUE);
doc.BOTH.s = prepString(c(ORIG,COPY), stemMe=TRUE);
```

### ORIG
```{r}
ORIG.freq.s = buildFreq(doc.ORIG.s);
set.seed(1234);
wordcloud2(ORIG.freq.s);
```


### COPY
```{r}
COPY.freq.s = buildFreq(doc.COPY.s);
set.seed(1234);
wordcloud2(COPY.freq.s);

```
## Summary

```{r}
TABLE.both = merge(ORIG.freq.s, COPY.freq.s, by="word", all.x=TRUE, all.y=TRUE);  # does this drop 0,1 ?
colnames(TABLE.both)=c("words","ORIG","COPY");
TABLE.both = TABLE.both[order(-TABLE.both$ORIG, -TABLE.both$COPY),];

TABLE.both[is.na(TABLE.both)] = 0; # NA from merge
TABLE.both;
```

## Cosine Similarity

### RAW
```{r}
tdm = TermDocumentMatrix(doc.BOTH);
crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))));
```

### STEMMED
```{r}
tdm = TermDocumentMatrix(doc.BOTH.s);
crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
```



# Latent Semantic Analysis

Find U,V,W and reduce "bag of words" to concepts.  The correlations should be higher.  

## Prep as matrix
```{r}
names.words = TABLE.both$words;
names.docs = colnames(TABLE.both)[-1];
X = t( TABLE.both[,-1] );
rownames(X) = names.docs;
colnames(X) = names.words;

dim(X);
head(X[,1:10]);
```

### Compute deviations
```{r}
X[,abs(X[1,] - X[2,]) > 3];
```


### Compute bigraph
```{r}
# prcomp vs princomp?
X.PCA = prcomp(t(X));  # this is only on STEM
summary(X.PCA);
str(X.PCA);
X.PCA;
biplot(X.PCA, 
       xlab=paste0("PC1 VAF: ",round(summary(X.PCA)$importance[2,1] * 100,1 ), "%"),
       ylab=paste0("PC2 VAF: ",round(summary(X.PCA)$importance[2,2] * 100,1 ), "%"),
      );
```

### Compute Cosine Similarity
```{r}
head(X[,1:10]); 
a = X[1,];  # ORIG
b = X[2,];  # COPY
theta = as.numeric( (a %*% b) / (sqrt(sum(a^2)) * sqrt(sum(b^2))) );  # cosine similarity of weighted vectors (TF-IDF)

theta;
```


## TF-IDF
```{r}
# https://www.mathworks.com/help/textanalytics/ref/bagofwords.tfidf.html

X.tf = colSums(X);  # term freq
X.idf = colSums(X != 0); # document freq

# NORM technique?
# https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency
X.tf.s = log(1 + X.tf);
X.idf.s = log( nrow(X) / (1+X.idf) ) + 1;

X.tf.idf.s = X.tf.s * X.idf.s;  # PAIRWISE products

# https://en.wikipedia.org/wiki/Latent_semantic_analysis


## scaled X
Xs = X * X.tf.idf.s # PAIRWISE products

```


### Compute deviations
```{r}
round(Xs[,abs(Xs[1,] - Xs[2,]) > 3],2);

round(Xs[,(Xs[1,] - Xs[2,]) < -1.5],2);
```

### Compute bigraph
```{r}
# prcomp vs princomp?
Xs.PCA = prcomp(t(Xs));  # this is only on STEM
summary(Xs.PCA);
str(Xs.PCA);
Xs.PCA;
biplot(Xs.PCA, 
       xlab=paste0("PC1 VAF: ",round(summary(Xs.PCA)$importance[2,1] * 100,1 ), "%"),
       ylab=paste0("PC2 VAF: ",round(summary(Xs.PCA)$importance[2,2] * 100,1 ), "%"),
      );
```

### Compute Cosine Similarity
```{r}
head(Xs[,1:10]);

a = Xs[1,];  # ORIG
b = Xs[2,];  # COPY
theta = as.numeric( (a %*% b) / (sqrt(sum(a^2)) * sqrt(sum(b^2))) );  # cosine similarity of weighted vectors (TF-IDF)

theta;
```




















# Conclusion

YES, emphatically.

```{r}

#which(names.words == "unalien");
#which(startsWith(names.words, "spir"));

#which(endsWith(names.words, "ip")); # relationship
                                    # why not STEMMED

# closer = c(1,2,4,8,9, 42, 61,62,34,13,36, 57,37,33,  76,55);
# from names.words
 closer = c(  which(names.words == "public"), which(names.words == "polit"), which(names.words == "british"), which(names.words == "sacr"), which(names.words == "friend"), which(names.words == "rule"), which(names.words == "system"), which(names.words == "trial"), which(names.words == "stand"), which(names.words == "pledg"), which(names.words == "rest"), which(names.words == "mercenari"), which(names.words == "life"), which(names.words == "truth"), which(names.words == "separ"), which(names.words == "judg"), which(names.words == "death"), which(names.words == "cours"), which(names.words == "creat"), which(names.words == "good"), which(names.words == "america"), which(names.words == "abolish"), which(names.words == "sea"), which(names.words == "king"), which(names.words == "men"), which(names.words == "live"), which(names.words == "histori"), which(names.words == "constitut"), which(names.words == "happi"), which(names.words == "common"), which(names.words == "britain"), which(names.words == "foreign"), which(names.words == "war"), which(names.words == "liberti"), which(names.words == "legislatur"), which(names.words == "coloni"), which(names.words == "establish"), which(names.words == "legisl"), which(names.words == "peopl"), which(names.words == "power"), which(names.words == "state"), which(names.words == "govern"), which(names.words == "right"), which(names.words == "law"), which(names.words == "time"),  which(names.words == "declar"), which(names.words == "independ"), which(names.words == "free"), which(names.words == "will"), which(names.words == "natur"), which(names.words == "great"), which(names.words == "will"),  which(names.words == "slaveri") );     

X[,closer];
round(Xs[,closer],2);



```

Attribution is a function of citing your sources, giving praise, honor, and glory to the appropriate party.

I am an open-source maniac (I like WTFPL over MIT licensing, but both are sufficient.) since technically all knowledge comes from Jehovah. This idea may be a potential research article for one of you.  Go for it.   At the end of the review process (final submission), just add a footnote of acknowledgment to "Nephi John" if you deem it appropriate.  Cheers and Aloha! 


# Appendix 

```{r}
# https://www.researchgate.net/publication/321804167
# https://stackoverflow.com/questions/29750519/

require(tm)
data("crude")
length(crude)

tdm <- TermDocumentMatrix(crude,
                          control = list(removePunctuation = TRUE,
                                         stopwords = TRUE))

library(slam)
cosine_dist_mat <- crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))



library(wordcloud);
wordcloud(words = COPY.freq.s$word, freq = COPY.freq.s$freq,
                        min.freq=1,
                        max.words=100,
                        random.order=FALSE,
                        rot.per = 0.25
            );

```



## SVD and PCA are equivalent
```{r}
# https://stats.stackexchange.com/questions/134282/
# some sloppy notation with V based on R naming


# covariance
C = Xs %*% t(Xs) / (nrow(Xs) - 1);  
C.eigen = eigen(C);
  W = C.eigen$vectors;  # renamed 
  L = diag(C.eigen$values);
C2 = W %*% L %*% t(W);

C == C2;
# R bug due to tolerance, shows some as FALSE
all.equal(as.numeric(C), as.numeric(C2) );
# actually TRUE for ALL

# SVD as X = U D t(V)
# X = U D V'  ... Xs == U %*% D %*% t(V); isTRUE(all.equal(as.numeric(Xs), as.numeric(U %*% D %*% t(V))) );
# D = U' X V  ... D == t(U) %*% Xs %*% V; isTRUE(all.equal(as.numeric(D), as.numeric(t(U) %*% Xs %*% V )) );
Xs.SVD = svd(Xs);  
str(Xs.SVD);

D = diag(Xs.SVD$d);  # this was called "S"
L == D^2;
isTRUE(all.equal(L, D^2));

U = Xs.SVD$u;  # Recall that the singular vectors are only defined up to sign
V = Xs.SVD$v;

U == W;
isTRUE(all.equal(U, W));
isTRUE(all.equal(abs(U), abs(W)));


```



I need to use one for U,V and the other for W?  These 'tweener' matrices are the maps between "concepts" and docs/words as suggested in the 'biplot'
